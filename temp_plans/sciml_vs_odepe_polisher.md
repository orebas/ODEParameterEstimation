# SciML vs ODEPE Polisher — Detailed Comparison & Action Items

## Context

The benchmarking harness (`~/tmp/no-matlab-no-worry/`) compares ODEPE against a "SciML estimator". Both use Optimization.jl + ForwardDiff to minimize sum-of-squared-errors, but they differ in several ways that could bias benchmark results. This document catalogs **every** aspect — both the differences and the things that are the same — so we can go through each one and decide what to do.

### Source files

| File | Role |
|------|------|
| `~/tmp/no-matlab-no-worry/templates/julia_template_for_estimation_sciml.jl` | SciML estimation template |
| `~/tmp/no-matlab-no-worry/templates/julia_template_for_estimation_odepe.jl` | ODEPE estimation template |
| `~/tmp/no-matlab-no-worry/config/config.json` | Benchmark configuration |
| `~/tmp/no-matlab-no-worry/src/generate_scripts.py` | Script generator (builds `data_expr` for SciML) |
| `~/tmp/no-matlab-no-worry/src/generate_data.py` | Data generator (randomizes true params/ICs) |
| `src/core/parameter_estimation.jl` ~line 1627 | ODEPE `_optimize_against_data` |
| `src/core/parameter_estimation.jl` ~line 1755 | ODEPE `polish_solution_using_optimization` |
| `src/core/parameter_estimation.jl` ~line 1798 | ODEPE `direct_optimization_parameter_estimation` |

---

## Complete aspect-by-aspect comparison

### SAME: Loss function

Both minimize **sum of squared errors (SSE)** over all observables and time points:

```
L(p) = Σ_obs Σ_time (simulated_obs(t) - data_obs(t))²
```

- SciML: `sum(sum((data[i] .- data_true[i]) .^ 2) for i in eachindex(data))`
- ODEPE: `Σⱼ Σᵢ (f(sol.u[i], params) - data_true[i])²` (manual loop)

**Verdict**: Identical objective. No action needed.

---

### SAME: AD backend

Both use **ForwardDiff** (forward-mode automatic differentiation):

- SciML: `Optimization.AutoForwardDiff()` (line 64)
- ODEPE: `get_ad_backend(opts.opt_ad_backend)` where default is `:forward` → `AutoForwardDiff()`

**Verdict**: Same. No action needed.

---

### SAME: Failed ODE handling

Both return `Inf` when the ODE solver fails or returns a non-Success retcode.

**Verdict**: Same. No action needed.

---

### SAME: Callback

Both use a no-op callback (always returns `false`, never stops early).

- SciML: `callback = function (p, l) return false end`
- ODEPE: no callback (unless `ODEPE_OPT_VERBOSE=true`, which adds logging but still returns `false`)

**Verdict**: Same. No action needed.

---

### SAME: Parameter vector layout

Both concatenate `[initial_conditions..., parameters...]` into a single flat vector:

- SciML: `p_rand = rand(Uniform(...), length(ic) + length(p_true))`
- ODEPE: `p0 = vcat(ic_vec, param_vec)`

**Verdict**: Same. No action needed.

---

### SAME: Data source

Both read the same CSV files generated by the benchmark harness. Both get identical noisy data for the same instance.

**Verdict**: Same. No action needed.

---

### SAME: Optimization framework

Both use `Optimization.jl` → `OptimizationOptimJL` for the solver backend.

**Verdict**: Same. No action needed.

---

### SAME: Observable evaluation (semantics)

Both evaluate measured quantities by substituting ODE solution states and parameters into the measurement expression:

- SciML: Template-generated code like `4.0.*sol[r]` (via `get_sciml_measurements()` in generate_scripts.py)
- ODEPE: `ModelingToolkit.build_function(eq.rhs, unknown_syms, param_syms)` compiled callable, evaluated as `f(sol.u[i], param_guess)`

Both correctly handle nonlinear observations (e.g., `y ~ 4.0*r`, `y ~ x1^2 + x2`).

**Verdict**: Semantically equivalent. Implementation differs (ODEPE compiles a callable, SciML inlines the expression). No action needed.

---

### DIFFERENT: Optimizer algorithm

| | SciML | ODEPE |
|---|---|---|
| Algorithm | `BFGS()` | `LBFGS()` (both polish and direct) |
| Hessian approx | Full dense matrix | Limited-memory (default m=10 vectors) |

For small problems (2–13 params in our benchmark), BFGS is slightly more effective — the full Hessian approximation costs O(n²) memory but gives better search directions. LBFGS saves memory but is less accurate for small n.

**Decision**: Switch ODEPE benchmark to `PolishBFGS` / `BFGS()` to match.

---

### DIFFERENT: ODE solver inside loss function

| | SciML | ODEPE |
|---|---|---|
| Solver | `Tsit5()` (5th order, explicit) | `AutoVern9(Rodas4P())` (9th order + implicit fallback) |
| Character | Faster per step, less accurate | More accurate per step, slower, handles stiff systems |

Since the loss is evaluated tens/hundreds of thousands of times, the per-evaluation solver cost dominates runtime. Tsit5 is ~2-5x cheaper per call than Vern9 at the same tolerance. However, AutoVern9(Rodas4P()) handles stiff systems that Tsit5 would fail on.

**Note**: SciML also loads `Vern9()` on line 12 and uses it for an initial simulation (line 42), but the loss function itself uses `Tsit5()` — so the optimization loop runs with the cheaper solver.

**Decision**: Keep AutoVern9(Rodas4P()) — it's a strength of ODEPE that it handles stiff systems. Document the difference.

---

### DIFFERENT: Tolerances

| | SciML | ODEPE (current benchmark template) |
|---|---|---|
| abstol | `1e-13` | `1e-14` (EstimationOptions default) |
| reltol | `1e-13` | `1e-14` (EstimationOptions default) |

ODEPE is 10x tighter. This makes ODEPE's ODE solves slightly slower and more accurate.

**Decision**: Align to 1e-13 in the benchmark template.

---

### DIFFERENT: Max iterations — **BIGGEST GAP**

| | SciML | ODEPE polish | ODEPE direct |
|---|---|---|---|
| maxiters | **200,000** | **50** (`polish_maxiters`) | **10,000** (`opt_maxiters`) |

SciML gets 4000x more iterations than the ODEPE polisher and 20x more than direct optimization. This is the single largest difference.

**Decision**: Set `polish_maxiters = 200000` and `opt_maxiters = 200000` in the benchmark template.

---

### DIFFERENT: Bounds

| | SciML | ODEPE |
|---|---|---|
| Lower bound | `0.0 * ones(N)` (from `SEARCH_BOUNDS[0]`) | `nothing` (unbounded) |
| Upper bound | `1.0 * ones(N)` (from `SEARCH_BOUNDS[1]`) | `nothing` (unbounded) |

SciML always uses box constraints. ODEPE polish is unbounded by default. This matters because:
- True parameters are generated in `[0.1, 0.9]` (from `PARAM_INTERVAL`)
- SciML's bounds `[0.0, 1.0]` constrain the search to a sensible region
- ODEPE without bounds can waste iterations exploring negative or very large values

**Decision**: Set `opt_lb`/`opt_ub` in the ODEPE benchmark template to match SciML's `[0.0, 1.0]` bounds via template variables.

---

### DIFFERENT: Initial guess — **FUNDAMENTAL DESIGN DIFFERENCE**

| | SciML | ODEPE (FlowStandard + polish) |
|---|---|---|
| Strategy | Single random point in `[0.0, 1.0]` | Algebraic solver → polish near-optimal solution |
| Quality | Blind (could be far from any solution) | Near a global solution (from HC/SI) |
| Multi-start | Via benchmark's 10 test instances per model (different random seeds) | Via multiple HC solutions (finds ALL algebraic solutions) |

This is the **core advantage** ODEPE demonstrates: the algebraic solver finds globally correct solutions, then polishing just refines numerical accuracy. SciML relies on gradient descent from a random start, which can get stuck in local optima.

**Verdict**: This difference is **by design** — it's what the paper benchmarks. No action.

---

### DIFFERENT: Number of random starts / retries

| | SciML | ODEPE |
|---|---|---|
| Per benchmark instance | 1 random start, 1 optimization run | 1 algebraic solve (finds all solutions), then 1 polish per solution |
| Across benchmark | 10 instances per model × 5 noise levels = 50 runs per model, each with different random true params AND different random start | Same 50 runs, but algebraic solver is deterministic given data |

Neither estimator does explicit multi-start within a single instance. SciML's "multi-start" comes from the benchmark running 10 independent instances with different random conditions.

**Verdict**: Document this. No action.

---

### DIFFERENT: SciMLSensitivity

SciML loads `using SciMLSensitivity` which can provide continuous adjoint/forward sensitivity analysis for more efficient gradient computation through the ODE solver. ODEPE does not load this — it relies on ForwardDiff differentiating through the discrete ODE solver steps.

For small systems (2–13 params), ForwardDiff is efficient. For larger systems, SciMLSensitivity would give a significant speedup.

**Verdict**: Not relevant for our benchmark's problem sizes. Document as a note.

---

### DIFFERENT: Random seed

Neither SciML nor ODEPE sets a random seed. Results are not reproducible across runs.

- SciML: `p_rand = rand(Uniform(...))` — unseeded
- ODEPE direct: `p0 = rand(p_size)` — unseeded (but irrelevant for FlowStandard since algebraic solver is deterministic)
- Data generation: `np.random.uniform(...)` — unseeded

**Decision**: Accept non-reproducibility for now (the benchmark harness generates data once and reuses it).

---

## Summary of changes made

### ODEPE benchmark template changes

File: `~/tmp/no-matlab-no-worry/templates/julia_template_for_estimation_odepe.jl`

| Setting | Old | New | Rationale |
|---------|-----|-----|-----------|
| `polish_maxiters` | 50 | 200000 | Match SciML's 200k iteration budget |
| `polish_method` | `PolishLBFGS` | `PolishBFGS` | Match SciML's BFGS (full Hessian) |
| `opt_maxiters` | (default 10000) | 200000 | Match SciML for any direct opt path |
| `opt_lb` | (nothing) | `{{lower_bound}} * ones(...)` | Match SciML [0,1] bounds |
| `opt_ub` | (nothing) | `{{upper_bound}} * ones(...)` | Match SciML [0,1] bounds |
| `abstol` | (default 1e-14) | 1e-13 | Match SciML tolerances |
| `reltol` | (default 1e-14) | 1e-13 | Match SciML tolerances |

### Kept different (by design)

| Setting | SciML | ODEPE | Rationale |
|---------|-------|-------|-----------|
| ODE solver | Tsit5() | AutoVern9(Rodas4P()) | ODEPE's stiffness handling is a feature |
| Initial guess | Random | Algebraic solver | Core benchmark comparison |
| SciMLSensitivity | Yes | No | Not relevant at benchmark scale |
